\documentclass[a4paper]{article}

\renewcommand{\familydefault}{\sfdefault}
\usepackage{parskip}
\usepackage[cm]{fullpage}
\usepackage{graphicx}
\usepackage{epstopdf}

\title{Agreement Scores}
\author{}
\date{}

\begin{document}
\maketitle

\section*{The Situation}
We have a set of audio transformations each described by their position in an 80 dimensional feature space, each dimension
corresponding to a particular feature of the transformation or the audio signals being transformed. Each of these transforms
is also labelled with a semantic descriptor (such as `warm', `crunch' or `fuzz'). Some sample data is shown in Figure
\ref{FeatureSpace}.

	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{FeatureSpace.eps}
		\caption{Two dimensions of the feature space.}
		\label{FeatureSpace}
	\end{figure}

What we want to measure is the level to which these semantic descriptors are agreed upon across the set of transforms they
label. Intuitively, if all the transforms labeled with are particular descriptor are group together in some meaningful way,
in the features space, that descriptor should receive a high agreement score. If the transforms labelled by a particular
descriptor are distributed across the entire feature space, the descriptor should receive a low agreement score.

A common approach to this is to sum the variances, in each dimension, for all of the transforms with a particular
descriptor.  The agreement for a descriptor, $d$, is then given by Equation \ref{socialeq}.

	\begin{equation}
		A(d) = \frac{\log(N_{d})}{\sum_{k=1}^{K} \sigma_{d,k}^{2}}
		\label{socialeq}
	\end{equation}

Where $N_{d}$ is the total number of transforms labelled with descriptor $d$, $K$ is the total number of features
(dimensions in the features space) and $\sigma_{d,k}^{2}$ is the variance, in feature $k$, for transforms labelled with
descriptor $d$.

I don't think this is a particularly good measure for a variety of reasons:

	\begin{itemize}
		\item The scaling by the log of the number of transforms doesn't seem very rigorous to me. Clearly the
			agreement score should increase if there are more transforms within the same region of the space,
			but this method doesn't seem statistically sound.
		\item It assumes that high variance, in any dimension, is a bad thing and should reduce the agreement
			score for a descriptor. It may be the case that a particular feature has no influence on the
			labelling of a transform and as such that feature should have little influence on the agreement
			score.
		\item It assumes that low variance is a good thing and should increase the agreement score. It may be the
			case that the value of a particular feature is the same for every transform, regardless of the
			descriptor used to label them. In this cast low variance should not influence the agreement score.
		\item It assumes that all features are independent. Summing the variances of each feature individually does
			not take into account the relationships between features. 
	\end{itemize}

Below are my thoughts on how we can improve on this measure.

\section*{Variance vs Margin of Error}
Firstly I think we should be using something a little more descriptive than just the raw variance of each feature. The
margin of error seems more appropriate to me. This would properly take into account the number of transforms and the
expected distribution of feature values.

\section*{Sum of Reciprocals}
Secondly, I think taking the reciprocal of the sum of variances measures the wrong thing. In Equation \ref{socialeq} a high
variance in any feature will reduce the agreement score, regardless of how many features exhibit a low variance. Summing the
individual reciprocals of the margin of error in each feature, as shown in Equation \ref{reciprocalmoe} makes more sense to
me.

	\begin{equation}
		A(d) = \sum_{k=1}^{K} \frac{1}{E_{d,k}}
		\label{reciprocalmoe}
	\end{equation}

Where $E_{d,k}$ is the margin of error, in feature $k$, for transforms labelled with descriptor $d$.

In this way features with a large margin of error have little effect on the overall agreement score, while features with a
small margin or error will increase it.

\section*{Using Principal Components}
Equation \ref{reciprocalmoe} still suffers from the problem of assuming a small margin of error is due to an agreement
between labels. We need to discard any features which have a small variance regardless of their descriptor. This can be done
using principal component analysis (PCA) on the entire feature space (including all transforms), and keeping only the first
$P$ principal components (PCs). If the transforms labelled with a particular descriptor exhibit low variance in one of these
first $P$ PCs it is likely that it is well agreed upon.

The problem now is that the PCs all describe different proportions of the variance in the original data.  This can be taken
into consideration by weighting the margin of error in each PC by the proportion of variance described by that PC. In a PC
which describes a large proportion of variance, a low variance is not expected and as such should be given a higher
weighting in the agreement score. In a PC which describes a low proportion of variance, a low variance is expected so the
weighting should be less. This gives rise to Equation \ref{weighting}.

	\begin{equation}
		A(d) = \sum_{p=1}^{P} \frac{V_{p}}{E_{d,p}}
		\label{weighting}
	\end{equation}

Where $V_{p}$ is the proportion of variance described by PC $p$ and $E_{d,p}$ is the margin of error, in PC $p$, for
transforms labelled with descriptor $d$.

\section*{Relationships Between PCs}
One problem still remains that a certain descriptor might exhibit some relationship between two PCs. The original PCA is
performed on the entire feature space so only features which are proportional for all transforms will be combined into
single PCs. It is possible that two features are only related for transforms with a particular descriptor, in which case
they will most likely be split into two separate PCs. This means that, for some descriptors, there may be relationships that
exist between PCs which we need to capture in our agreement score.

My idea for this is to find the covariance matrix for transforms labelled with a particular descriptor in the first $P$ PCs,
then use its eigenvalues and eigenvectors in the calculation of the agreement score. Something like Equation \ref{eigen}.

	\begin{equation}
		A(d) = \sum_{i=1}^{P} \frac{V \cdot v_{d,i}}{\lambda_{d,i}}
		\label{eigen}
	\end{equation}

Where $V$ is a vector containing the proportions of variances described by the first $P$ PCs, $v_{i}$ is the
$i^{\textrm{th}}$ eigenvector of the covariance matrix for transforms labelled with descriptor $d$ and $\lambda_{d,i}$ is
the corresponding eigenvalue.

The numerator in Equation \ref{eigen} is the equivalent of weighting by the PC's proportions of variances in Equation
\ref{weighting}. Taking the dot product of the eigenvector and a vector containing the PC's proportions of variance takes
into account the weighted variance described by the PCs to which the eigenvector is not orthogonal.

The use of eigenvalues could probably be improved by using a margin of error equivalent, but I'm not sure on what
distribution one would use in the calculation of such error measures.

\section*{Questions}
So, that is what I have thought about and here are some questions I have about it.

	\begin{enumerate}
		\item Does any of what I have said make any sense whatsoever?
		\item Are there any existing statistical measures for this type of thing? I have looked at various inter
			rater reliability measures but none of them seem to apply.
		\item What are your thoughts on the whole eigenvector weighting thing (Equation \ref{eigen})? It seems to
			make sense to me but maybe there are some gaping holes in my reasoning.
		\item Any other suggestions you may have.
	\end{enumerate}

Thanks.

\end{document}
